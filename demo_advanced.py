"""
DÃ©monstration avancÃ©e avec 5 modÃ¨les et donnÃ©es diversifiÃ©es
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from data.data_loader import DataLoader
from data.data_generator import DataGenerator
from data.preprocessor import DataPreprocessor
from models.linear_regression_model import LinearRegressionModel
from models.random_forest_model import RandomForestModel
from models.xgboost_model import XGBoostModel
from models.gradient_boosting_model import GradientBoostingModel
from models.support_vector_model import SupportVectorModel
from evaluation.evaluator import ModelEvaluator
from visualization.visualizer import DataVisualizer
from utils.logger import setup_logger
import pandas as pd
import numpy as np
import time

def main():
    print("=" * 80)
    print("ğŸš€ DÃ‰MONSTRATION AVANCÃ‰E - 5 MODÃˆLES & DONNÃ‰ES DIVERSIFIÃ‰ES ğŸš€")
    print("=" * 80)
    
    logger = setup_logger()
    
    # 1. Test de diffÃ©rentes sources de donnÃ©es
    print("\nğŸ“Š 1. TEST DE DIFFÃ‰RENTES SOURCES DE DONNÃ‰ES")
    print("-" * 60)
    
    data_sources = ['mixed', 'california', 'synthetic']
    datasets = {}
    
    for source in data_sources:
        print(f"\nğŸ” Test de la source: {source}")
        try:
            data_loader = DataLoader(use_database=False, data_source=source)
            X, y = data_loader.load_boston_housing()
            
            # Informations sur le dataset
            generator = DataGenerator()
            info = generator.get_dataset_info(X, y)
            
            datasets[source] = {
                'X': X, 'y': y, 'info': info,
                'loader': data_loader
            }
            
            print(f"âœ… {source.capitalize()}: {info['n_samples']} Ã©chantillons, {info['n_features']} features")
            print(f"   Prix: {info['price_mean']:.0f}â‚¬ Â± {info['price_std']:.0f}â‚¬")
            
        except Exception as e:
            print(f"âŒ Erreur avec {source}: {e}")
    
    # Choisir le meilleur dataset (le plus grand)
    if datasets:
        best_source = max(datasets.keys(), key=lambda k: datasets[k]['info']['n_samples'])
        X, y = datasets[best_source]['X'], datasets[best_source]['y']
        data_loader = datasets[best_source]['loader']
        
        print(f"\nğŸ† Dataset sÃ©lectionnÃ©: {best_source}")
        print(f"   ğŸ“Š {len(X)} Ã©chantillons, {len(X.columns)} features")
        print(f"   ğŸ’° Prix moyen: {y.mean():.0f}â‚¬")
    else:
        print("âŒ Aucun dataset disponible, arrÃªt de la dÃ©monstration")
        return
    
    # 2. Visualisation des donnÃ©es
    print("\nğŸ“ˆ 2. VISUALISATION DES DONNÃ‰ES")
    print("-" * 60)
    
    try:
        visualizer = DataVisualizer()
        print("âœ… GÃ©nÃ©ration des graphiques d'exploration...")
        # visualizer.plot_data_distribution(X, y)  # CommentÃ© pour Ã©viter l'affichage
    except Exception as e:
        print(f"âš ï¸  Erreur de visualisation: {e}")
    
    # 3. Preprocessing
    print("\nğŸ”§ 3. PREPROCESSING DES DONNÃ‰ES")
    print("-" * 60)
    
    preprocessor = DataPreprocessor()
    X_train, X_test, y_train, y_test = preprocessor.prepare_data(X, y, test_size=0.2)
    
    print(f"âœ… DonnÃ©es prÃ©parÃ©es:")
    print(f"   ğŸ¯ EntraÃ®nement: {X_train.shape[0]} Ã©chantillons")
    print(f"   ğŸ§ª Test: {X_test.shape[0]} Ã©chantillons")
    print(f"   ğŸ“Š Features: {X_train.shape[1]}")
    
    # 4. EntraÃ®nement des 5 modÃ¨les
    print("\nğŸ¤– 4. ENTRAÃNEMENT DE 5 MODÃˆLES AVANCÃ‰S")
    print("-" * 60)
    
    models = {
        'Linear Regression': LinearRegressionModel(),
        'Random Forest': RandomForestModel(n_estimators=100),
        'XGBoost': XGBoostModel(n_estimators=100),
        'Gradient Boosting': GradientBoostingModel(n_estimators=100),
        'Support Vector': SupportVectorModel(kernel='rbf', C=1.0)
    }
    
    evaluator = ModelEvaluator()
    results = {}
    predictions_dict = {}
    training_times = {}
    
    print("ğŸš€ EntraÃ®nement en cours...")
    
    for name, model in models.items():
        print(f"\n   ğŸ”„ {name}...")
        
        start_time = time.time()
        
        try:
            # EntraÃ®ner le modÃ¨le
            model.train(X_train, y_train)
            
            # Faire des prÃ©dictions
            predictions = model.predict(X_test)
            
            # Ã‰valuer
            metrics = evaluator.evaluate(y_test, predictions)
            
            # Stocker les rÃ©sultats
            results[name] = metrics
            predictions_dict[name] = predictions
            training_times[name] = time.time() - start_time
            
            print(f"   âœ… {name}: RMSE={metrics['RMSE']:.4f}, RÂ²={metrics['R2']:.4f} ({training_times[name]:.2f}s)")
            
            # Sauvegarder en base si disponible
            if hasattr(data_loader, 'save_model_results_to_db'):
                hyperparams = {}
                if hasattr(model.model, 'get_params'):
                    hyperparams = model.model.get_params()
                
                data_loader.save_model_results_to_db(
                    model_name=name,
                    model_version="2.0",
                    metrics=metrics,
                    hyperparameters=hyperparams,
                    training_samples=len(X_train),
                    test_samples=len(X_test)
                )
            
        except Exception as e:
            print(f"   âŒ Erreur avec {name}: {e}")
            # Continuer avec les autres modÃ¨les
            continue
    
    # 5. Comparaison des rÃ©sultats
    print("\nğŸ“Š 5. COMPARAISON DES PERFORMANCES")
    print("-" * 60)
    
    if results:
        # CrÃ©er un DataFrame des rÃ©sultats
        results_df = pd.DataFrame(results).T
        results_df['Training_Time'] = [training_times.get(model, 0) for model in results_df.index]
        
        print("\n=== TABLEAU COMPLET DES RÃ‰SULTATS ===")
        print(results_df.round(4))
        
        # Analyse des performances
        best_rmse = results_df['RMSE'].idxmin()
        best_r2 = results_df['R2'].idxmax()
        fastest = results_df['Training_Time'].idxmin()
        
        print(f"\nğŸ† ANALYSE DES PERFORMANCES:")
        print(f"   ğŸ¯ Meilleur RMSE: {best_rmse} ({results_df.loc[best_rmse, 'RMSE']:.4f})")
        print(f"   ğŸ“ˆ Meilleur RÂ²: {best_r2} ({results_df.loc[best_r2, 'R2']:.4f})")
        print(f"   âš¡ Plus rapide: {fastest} ({results_df.loc[fastest, 'Training_Time']:.2f}s)")
        
        # Graphiques de comparaison
        try:
            evaluator.compare_models(results)
            # visualizer.plot_model_comparison(results)  # CommentÃ© pour Ã©viter l'affichage
        except Exception as e:
            print(f"âš ï¸  Erreur de visualisation: {e}")
    
    # 6. Analyse dÃ©taillÃ©e du meilleur modÃ¨le
    print("\nğŸ” 6. ANALYSE DÃ‰TAILLÃ‰E DU MEILLEUR MODÃˆLE")
    print("-" * 60)
    
    if results:
        best_model_name = results_df['RMSE'].idxmin()
        best_predictions = predictions_dict[best_model_name]
        best_model = models[best_model_name]
        
        print(f"ğŸ† ModÃ¨le sÃ©lectionnÃ©: {best_model_name}")
        
        # Importance des features (si disponible)
        feature_importance = best_model.get_feature_importance()
        if feature_importance and 'feature_importance' in feature_importance:
            importance_df = pd.DataFrame({
                'Feature': X_train.columns,
                'Importance': feature_importance['feature_importance']
            }).sort_values('Importance', ascending=False)
            
            print("\nğŸ“Š Top 5 Features les plus importantes:")
            for i, row in importance_df.head().iterrows():
                print(f"   {row['Feature']}: {row['Importance']:.4f}")
        
        # Quelques prÃ©dictions d'exemple
        print(f"\nğŸ¯ Exemples de prÃ©dictions ({best_model_name}):")
        for i in range(min(5, len(best_predictions))):
            actual = y_test.iloc[i]
            predicted = best_predictions[i]
            error = abs(actual - predicted)
            error_pct = (error / actual) * 100
            
            print(f"   PropriÃ©tÃ© {i+1}: {predicted:.0f}â‚¬ (rÃ©el: {actual:.0f}â‚¬, erreur: {error_pct:.1f}%)")
            
            # Sauvegarder quelques prÃ©dictions
            if hasattr(data_loader, 'save_prediction_to_db'):
                confidence = max(0, 1 - (error / actual))
                data_loader.save_prediction_to_db(
                    model_name=best_model_name,
                    predicted_price=predicted,
                    actual_price=actual,
                    model_version="2.0",
                    confidence_score=confidence
                )
    
    # 7. Recommandations
    print("\nğŸ’¡ 7. RECOMMANDATIONS")
    print("-" * 60)
    
    if results:
        print("ğŸ“‹ Analyse des rÃ©sultats:")
        
        # Analyser les performances
        avg_rmse = results_df['RMSE'].mean()
        avg_r2 = results_df['R2'].mean()
        
        if avg_r2 > 0.8:
            print("   âœ… Excellentes performances globales (RÂ² > 0.8)")
        elif avg_r2 > 0.6:
            print("   âœ… Bonnes performances globales (RÂ² > 0.6)")
        else:
            print("   âš ï¸  Performances moyennes, considÃ©rer:")
            print("      â€¢ Plus de donnÃ©es d'entraÃ®nement")
            print("      â€¢ Feature engineering avancÃ©")
            print("      â€¢ Hyperparameter tuning plus poussÃ©")
        
        # Recommandations par modÃ¨le
        if 'XGBoost' in results and results['XGBoost']['R2'] > 0.7:
            print("   ğŸš€ XGBoost recommandÃ© pour la production")
        elif 'Random Forest' in results and results['Random Forest']['R2'] > 0.7:
            print("   ğŸŒ² Random Forest recommandÃ© (bon compromis performance/interprÃ©tabilitÃ©)")
        elif 'Linear Regression' in results and results['Linear Regression']['R2'] > 0.8:
            print("   ğŸ“ˆ Linear Regression surprenamment efficace (donnÃ©es linÃ©aires)")
    
    # 8. Sauvegarde des rÃ©sultats
    print("\nğŸ’¾ 8. SAUVEGARDE DES RÃ‰SULTATS")
    print("-" * 60)
    
    if results:
        # Sauvegarder les rÃ©sultats dÃ©taillÃ©s
        results_df.to_csv('data/processed/advanced_model_results.csv')
        print("âœ… RÃ©sultats sauvegardÃ©s dans data/processed/advanced_model_results.csv")
        
        # CrÃ©er un rapport de synthÃ¨se
        summary = {
            'dataset_source': best_source,
            'n_samples': len(X),
            'n_features': len(X.columns),
            'best_model': best_model_name,
            'best_rmse': results_df.loc[best_model_name, 'RMSE'],
            'best_r2': results_df.loc[best_model_name, 'R2'],
            'avg_performance': avg_r2,
            'models_tested': len(results)
        }
        
        summary_df = pd.DataFrame([summary])
        summary_df.to_csv('data/processed/advanced_summary.csv', index=False)
        print("âœ… RÃ©sumÃ© sauvegardÃ© dans data/processed/advanced_summary.csv")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ DÃ‰MONSTRATION AVANCÃ‰E TERMINÃ‰E AVEC SUCCÃˆS ! ğŸ‰")
    print("=" * 80)
    
    if results:
        print(f"ğŸ† Meilleur modÃ¨le: {best_model_name}")
        print(f"ğŸ“Š Dataset utilisÃ©: {best_source} ({len(X)} Ã©chantillons)")
        print(f"ğŸ¯ Performance: RMSE={results_df.loc[best_model_name, 'RMSE']:.4f}, RÂ²={results_df.loc[best_model_name, 'R2']:.4f}")
        print(f"ğŸš€ {len(results)} modÃ¨les testÃ©s avec succÃ¨s")

if __name__ == "__main__":
    main()