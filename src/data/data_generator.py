"""
GÃ©nÃ©rateur de donnÃ©es diversifiÃ©es pour tester les modÃ¨les
"""
import pandas as pd
import numpy as np
import requests
from typing import Tuple, Optional
from sklearn.datasets import make_regression, fetch_california_housing
import warnings

class DataGenerator:
    """GÃ©nÃ©rateur de datasets diversifiÃ©s"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        np.random.seed(random_state)
    
    def generate_synthetic_housing_data(self, n_samples: int = 1000, 
                                      complexity: str = 'medium') -> Tuple[pd.DataFrame, pd.Series]:
        """
        GÃ©nÃ¨re des donnÃ©es synthÃ©tiques de maisons avec diffÃ©rents niveaux de complexitÃ©
        
        Args:
            n_samples: Nombre d'Ã©chantillons
            complexity: 'simple', 'medium', 'complex'
            
        Returns:
            Tuple[pd.DataFrame, pd.Series]: Features et target
        """
        print(f"ðŸ—ï¸ GÃ©nÃ©ration de {n_samples} propriÃ©tÃ©s synthÃ©tiques (niveau: {complexity})")
        
        if complexity == 'simple':
            n_features = 8
            noise = 0.05
        elif complexity == 'medium':
            n_features = 13
            noise = 0.1
        else:  # complex
            n_features = 20
            noise = 0.15
        
        # GÃ©nÃ©ration de base
        X, y = make_regression(
            n_samples=n_samples,
            n_features=n_features,
            noise=noise,
            random_state=self.random_state
        )
        
        # Noms des features selon la complexitÃ©
        if complexity == 'simple':
            feature_names = ['surface', 'chambres', 'age', 'distance_centre', 
                           'score_quartier', 'garage', 'jardin', 'etage']
        elif complexity == 'medium':
            feature_names = ['surface', 'chambres', 'salles_bain', 'age', 'distance_centre',
                           'score_quartier', 'garage', 'jardin', 'etage', 'balcon',
                           'cave', 'ascenseur', 'chauffage']
        else:  # complex
            feature_names = ['surface', 'chambres', 'salles_bain', 'age', 'distance_centre',
                           'score_quartier', 'garage', 'jardin', 'etage', 'balcon',
                           'cave', 'ascenseur', 'chauffage', 'isolation', 'securite',
                           'transport_public', 'commerces', 'ecoles', 'hopitaux', 'pollution']
        
        # CrÃ©er le DataFrame
        X_df = pd.DataFrame(X, columns=feature_names)
        
        # Normaliser et ajuster les valeurs pour qu'elles soient rÃ©alistes
        for i, col in enumerate(X_df.columns):
            if 'surface' in col:
                X_df[col] = np.abs(X_df[col]) * 50 + 50  # 50-500 mÂ²
            elif 'chambres' in col:
                X_df[col] = np.abs(X_df[col]) % 6 + 1  # 1-6 chambres
            elif 'age' in col:
                X_df[col] = np.abs(X_df[col]) * 30 + 5  # 5-95 ans
            elif 'distance' in col:
                X_df[col] = np.abs(X_df[col]) * 20 + 1  # 1-40 km
            elif any(word in col for word in ['score', 'quartier', 'securite']):
                X_df[col] = (X_df[col] - X_df[col].min()) / (X_df[col].max() - X_df[col].min()) * 10  # 0-10
            else:
                # Variables binaires ou catÃ©gorielles
                X_df[col] = (X_df[col] > X_df[col].median()).astype(int)
        
        # Ajuster les prix de maniÃ¨re rÃ©aliste
        y_series = pd.Series(y, name='prix')
        
        # Formule plus rÃ©aliste basÃ©e sur les features principales
        realistic_price = (
            X_df['surface'] * 3000 +  # 3000â‚¬/mÂ²
            X_df['chambres'] * 15000 +  # 15000â‚¬ par chambre
            (100 - X_df['age']) * 500 +  # DÃ©prÃ©ciation avec l'Ã¢ge
            X_df['score_quartier'] * 10000 +  # Impact du quartier
            np.random.normal(0, 20000, len(X_df))  # VariabilitÃ©
        )
        
        # MÃ©langer avec les donnÃ©es gÃ©nÃ©rÃ©es pour garder de la complexitÃ©
        y_final = 0.7 * realistic_price + 0.3 * ((y - y.min()) / (y.max() - y.min()) * 200000 + 100000)
        y_series = pd.Series(y_final, name='prix')
        
        print(f"âœ… DonnÃ©es gÃ©nÃ©rÃ©es: prix moyen {y_series.mean():.0f}â‚¬, Ã©cart-type {y_series.std():.0f}â‚¬")
        return X_df, y_series
    
    def load_california_housing(self) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Charge le dataset California Housing de scikit-learn
        
        Returns:
            Tuple[pd.DataFrame, pd.Series]: Features et target
        """
        print("ðŸ–ï¸ Chargement du dataset California Housing...")
        
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                housing = fetch_california_housing()
            
            X = pd.DataFrame(housing.data, columns=housing.feature_names)
            y = pd.Series(housing.target * 100000, name='prix')  # Convertir en euros
            
            print(f"âœ… California Housing chargÃ©: {len(X)} propriÃ©tÃ©s")
            print(f"   Prix moyen: {y.mean():.0f}â‚¬, Ã©cart-type: {y.std():.0f}â‚¬")
            
            return X, y
            
        except Exception as e:
            print(f"âŒ Erreur lors du chargement California Housing: {e}")
            print("ðŸ”„ Basculement vers donnÃ©es synthÃ©tiques")
            return self.generate_synthetic_housing_data(n_samples=20640, complexity='medium')
    
    def load_online_housing_data(self) -> Optional[Tuple[pd.DataFrame, pd.Series]]:
        """
        Tente de charger des donnÃ©es immobiliÃ¨res depuis une source en ligne
        
        Returns:
            Optional[Tuple[pd.DataFrame, pd.Series]]: Features et target ou None
        """
        print("ðŸŒ Tentative de chargement de donnÃ©es en ligne...")
        
        # URLs de datasets publics
        urls = [
            "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv",
            "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
        ]
        
        for i, url in enumerate(urls):
            try:
                print(f"   Essai {i+1}: {url.split('/')[-1]}")
                
                df = pd.read_csv(url)
                
                if len(df) > 100:  # VÃ©rifier que le dataset est valide
                    # Identifier la colonne de prix (plusieurs noms possibles)
                    price_columns = ['medv', 'price', 'median_house_value', 'target', 'y']
                    price_col = None
                    
                    for col in price_columns:
                        if col in df.columns:
                            price_col = col
                            break
                    
                    if price_col:
                        X = df.drop(columns=[price_col])
                        y = df[price_col]
                        
                        # Nettoyer les donnÃ©es
                        X = X.select_dtypes(include=[np.number])  # Garder seulement les colonnes numÃ©riques
                        
                        if len(X.columns) >= 5:  # Au moins 5 features
                            print(f"âœ… DonnÃ©es en ligne chargÃ©es: {len(df)} Ã©chantillons")
                            print(f"   Features: {list(X.columns)}")
                            print(f"   Prix moyen: {y.mean():.2f}, Ã©cart-type: {y.std():.2f}")
                            
                            return X, pd.Series(y.values, name='prix')
                
            except Exception as e:
                print(f"   âŒ Ã‰chec: {e}")
                continue
        
        print("âŒ Impossible de charger des donnÃ©es en ligne")
        return None
    
    def generate_mixed_dataset(self, n_samples: int = 1500) -> Tuple[pd.DataFrame, pd.Series]:
        """
        GÃ©nÃ¨re un dataset mixte combinant plusieurs sources
        
        Args:
            n_samples: Nombre total d'Ã©chantillons souhaitÃ©s
            
        Returns:
            Tuple[pd.DataFrame, pd.Series]: Features et target combinÃ©es
        """
        print(f"ðŸŽ­ GÃ©nÃ©ration d'un dataset mixte ({n_samples} Ã©chantillons)")
        
        datasets = []
        
        # 1. Essayer les donnÃ©es en ligne
        online_data = self.load_online_housing_data()
        if online_data:
            X_online, y_online = online_data
            if len(X_online) > 0:
                # Prendre un Ã©chantillon
                sample_size = min(len(X_online), n_samples // 3)
                idx = np.random.choice(len(X_online), sample_size, replace=False)
                datasets.append((X_online.iloc[idx], y_online.iloc[idx], "online"))
        
        # 2. California Housing
        try:
            X_cal, y_cal = self.load_california_housing()
            sample_size = min(len(X_cal), n_samples // 3)
            idx = np.random.choice(len(X_cal), sample_size, replace=False)
            datasets.append((X_cal.iloc[idx], y_cal.iloc[idx], "california"))
        except:
            pass
        
        # 3. DonnÃ©es synthÃ©tiques pour complÃ©ter
        remaining_samples = n_samples - sum(len(X) for X, y, source in datasets)
        if remaining_samples > 0:
            X_synth, y_synth = self.generate_synthetic_housing_data(
                n_samples=remaining_samples, 
                complexity='complex'
            )
            datasets.append((X_synth, y_synth, "synthetic"))
        
        if not datasets:
            # Fallback: tout synthÃ©tique
            return self.generate_synthetic_housing_data(n_samples=n_samples, complexity='medium')
        
        # Combiner tous les datasets
        print("ðŸ”„ Combinaison des sources de donnÃ©es...")
        
        # Trouver les colonnes communes
        all_columns = set()
        for X, y, source in datasets:
            all_columns.update(X.columns)
        
        # Prendre les colonnes les plus communes (au moins dans 2 sources)
        column_counts = {}
        for X, y, source in datasets:
            for col in X.columns:
                column_counts[col] = column_counts.get(col, 0) + 1
        
        common_columns = [col for col, count in column_counts.items() if count >= 1]
        common_columns = common_columns[:15]  # Limiter Ã  15 features max
        
        # Standardiser et combiner
        combined_X_list = []
        combined_y_list = []
        
        for X, y, source in datasets:
            # SÃ©lectionner et rÃ©ordonner les colonnes
            available_cols = [col for col in common_columns if col in X.columns]
            X_subset = X[available_cols].copy()
            
            # Ajouter les colonnes manquantes avec des valeurs par dÃ©faut
            for col in common_columns:
                if col not in X_subset.columns:
                    X_subset[col] = np.random.normal(0, 1, len(X_subset))
            
            # RÃ©ordonner les colonnes
            X_subset = X_subset[common_columns]
            
            combined_X_list.append(X_subset)
            combined_y_list.append(y)
            
            print(f"   ðŸ“Š {source}: {len(X_subset)} Ã©chantillons")
        
        # ConcatÃ©ner
        final_X = pd.concat(combined_X_list, ignore_index=True)
        final_y = pd.concat(combined_y_list, ignore_index=True)
        
        # Normaliser les prix pour qu'ils soient cohÃ©rents
        final_y = (final_y - final_y.min()) / (final_y.max() - final_y.min()) * 400000 + 100000
        final_y.name = 'prix'
        
        print(f"âœ… Dataset mixte crÃ©Ã©: {len(final_X)} Ã©chantillons, {len(final_X.columns)} features")
        print(f"   Prix moyen: {final_y.mean():.0f}â‚¬, Ã©cart-type: {final_y.std():.0f}â‚¬")
        
        return final_X, final_y
    
    def get_dataset_info(self, X: pd.DataFrame, y: pd.Series) -> dict:
        """
        Retourne des informations sur le dataset
        
        Args:
            X: Features
            y: Target
            
        Returns:
            dict: Informations sur le dataset
        """
        return {
            'n_samples': len(X),
            'n_features': len(X.columns),
            'feature_names': list(X.columns),
            'price_mean': y.mean(),
            'price_std': y.std(),
            'price_min': y.min(),
            'price_max': y.max(),
            'missing_values': X.isnull().sum().sum(),
            'data_types': X.dtypes.value_counts().to_dict()
        }